 #### A.  Describe **one** question or decision that you will address using the data set you chose. The summarized question or decision must be relevant to a realistic organizational need or situation.  
What are the critical differences between customers who are staying with the company to those who are churning.
 
Testing 
#### B.  Describe the variables in the data set and indicate the specific type of data being described. Use examples from the data set that support your claims.
+ **Churn:** Whether the customer discontinued service within the last month (yes, no)
+ **CaseOrder:**A placeholder variable to preserve the original order of the raw data file. 
+ **Customer_id:**Unique customer ID (Primary key)
+   **Interaction, UID:**Unique IDs related to customer transactions, technical support, and sign-ups
###### The following variables represent customer demographic data:
 +  **City:** Customer city of residence as listed on the billing statement
 +   **State:** Customer state of residence as listed on the billing statement
 +   **County:** Customer county of residence as listed on the billing statement
 +    **Zip:** Customer zip code of residence as listed on the billing statement
 +    **Lat, Lng:** GPS coordinates of customer residence as listed on the billing statement
 +    **Population:** Population within a mile radius of customer, based on census data
 +    **Area:** Area type (rural, urban, suburban), based on census data
 +    **TimeZone:** Time zone of customer residence based on customer’s sign-up information
 +    **Job:** Job of the customer (or invoiced person) as reported in sign-up information
 +   **Children:** Number of children in customer’s household as reported in sign-up information +
 +   **Age:** Age of customer as reported in sign-up information
 +   **Education** : Highest degree earned by customer as reported in sign-up information
 +   **Employment** : Employment status of customer as reported in sign-up information
 +   **Income**: Annual income of customer as reported at time of sign-up
 +   **Marital:** Marital status of customer as reported in sign-up information
 +    **Gender:** Customer self-identification as male, female, or nonbinary
 +    **Outage_sec_perweek:** Average number of seconds per week of system outagesin the customer’s neighborhood
 
###### The following represent psychographics  data:
-   **Email:** Number of emails sent to the customer in the last year (marketing or correspondence)
-   **Contacts:** Number of times customer contacted technical support
-   **Yearly_equip_failure:** The number of times customer’s equipment failed and had to be reset/replaced in the past year
-   **Techie:** Whether the customer considers themselves technically inclined (based on customer questionnaire when they signed up for services) (yes, no)
-   **Contract:** The contract term of the customer (month-to-month, one year, two year)
-    **Port_modem:** Whether the customer has a portable modem (yes, no)
-   **Tablet:** Whether the customer owns a tablet such as iPad, Surface, etc. (yes, no)
-   **InternetService:** Customer’s internet service provider (DSL, fiber optic, None)
-   **Phone:** Whether the customer has a phone service (yes, no)
-   **Multiple:** Whether the customer has multiple lines (yes, no)
-   **OnlineSecurity:** Whether the customer has an online security add-on (yes, no)
-   **OnlineBackup:** Whether the customer has an online backup add-on (yes, no)
-   **DeviceProtection:** Whether the customer has device protection add-on (yes, no)
-   **TechSupport:** Whether the customer has a technical support add-on (yes, no)
-   **StreamingTV:** Whether the customer has streaming TV (yes, no)
-   **StreamingMovies:** Whether the customer has streaming movies (yes, no)
-   **PaperlessBilling:** Whether the customer has paperless billing (yes, no)
-   **PaymentMethod:** The customer’s payment method (electronic check, mailed check, bank (automatic bank transfer), credit card (automatic))
-   **Tenure**: Number of months the customer has stayed with the provider
-   **MonthlyCharge:** The amount charged to the customer monthly. This value reflects an average per customer.
-   **Bandwidth_GB_Year:** The average amount of data used, in GB, in a year by the customer


###### The following variables represent responses to an eight-question survey asking customers to rate the importance of various factors/surfaces on a scale of 1 to 8 (1 = most important, 8 = least important)
-   **Item1:** Timely response
-   **Item2:** Timely fixes
-   **Item3:** Timely replacements
-   **Item4:** Reliability
-   **Item5:** Options
-   **Item6:** Respectful response
-   **Item7:** Courteous exchange
-   **Item8:** Evidence of active listening


#### C.  Explain the plan for cleaning the data by doing the following:
- Propose a plan that includes the relevant techniques and specific steps needed to identify anomalies in the data set.

For continuous and discrete values, I will apply histograms and boxplots visualizations. The techniques I plan on using are histograms graphs, a variety of box plot graphs. Histogram graphs allow the data analyst to see the frequency of distribution.  Also, histograms will enable you to know where the mean, median, and mode lie upon on a distribution scale.  I also plan to use standard deviation to help understand the frequency of the data. In a normal distribution, it is believed that 68% of the values are within one standard deviation, 95% of the data values are within two standard deviations, and 99.7% of the values are within three standard deviations. Anything beyond three standard deviations will be considered an outlier and will be removed from the data. If the column does not have a normal distribution, I will resample the data to try to find a normal distribution to apply the three standard deviations technique. If I have too many values lying outside the standard deviations, I will use the winsorize technique instead of removing the outliers. The winsorize set outliers to a specific percentile. It will not be suitable to set outliers to the mean, median, or mode because we want the columns to be as unique as possible.  

Box plots allow you to see the quartiles and the IQR. I will use the violin plot to visualize the density of the distribution while enabling me to visualize the quartiles. 

If I have a few outliers in a column, I will remove the rows since it will not affect the prediction. 

For categorical values, I will evaluate them by distribution and the uniqueness of the values. I will use bar plots to visualize the categorical columns. If the columns only have one value, I will remove them since they will not provide any unique information. 

#### - Justify your approach for assessing the quality of the data, include:
- characteristics of the data being assessed
	- I will verify that there are no duplicate rows and the accuracy of each row.
	- If columns do not have granularity and uniqueness, I will remove them.
	- If the column contains null values, I will try to fill them with the median since it is more robust. Also, I will add binary columns because we do not why the columns have null values. It could be that the customer did not feel comfortable giving us the information, or the data never entered. Either way, null values still might contain valuable information, and filling them only with the median will erase some data we might use for the prediction.


#### - Justify your selected programming language and any libraries and packages that will support the data-cleaning process.
I will use python to code and visualize. Python has the following libraries that I will use to find anomalies:
- pandas - pandas allow you to make data frames from CSV files. It will enable you to manipulate the data.
- matplotlib - allows you to make visualizations to identify anomalies
- Seaborn - Seaborn is built on top of matplotlib to make even better visualizations with less code.
- scipy.stats - allows me to use trim_mean to trim the mean from both tails me to get the mean of the column while eliminating the anomalies 
- statsmodels - has robust stat functions that allow me to use the power of stats to find anomalies and better understand the data. 
- wquantiles - allows you to find quartiles of the column 

I will use Tableau to verify that zip, latitude, and longitude were entered correctly.

##### -  Provide the code you will use to identify the anomalies in the data.
- ipynb files allow with the Tableau workbook. 



#### D.  Summarize the data-cleaning process by doing the following:
###### 1. Describe the findings, including all anomalies, from the implementation of the data-cleaning plan from part C.
While exploring the data, I found out that the following columns had null values.
![[Pasted image 20210415182910.png]]
	**Lets take a look at the children column first**
	![[Pasted image 20210415191445.png]]
To help me fill the children column, I tried to see if the education column correlated with the children column. So, I created the following data frame.
![[Pasted image 20210415191814.png]]
I could not find a clear-cut correlation from the column, unfortunately. From the count plot above, I could see that median and mean had a gap between them. The Central limit theorem states that if you grab a sample from the column, you will find a normal distribution histogram. 
The histogram below is the original histogram without sampling the data.
![[Pasted image 20210415192617.png]]
The histogram below is what the histogram looks like after sampling the data.
![[Pasted image 20210415192806.png]]
![[Pasted image 20210415193040.png]]
As you can see now, the children column now has mean and median both towards the value of 2. I will fill the null values with two instead of 2.100 because it is not a realistic value and a discrete value. Also, I will create a binary column for all those values that contain had null values. As I stated above, maybe the parents did not feel comfortable sharing how many children they had. Filling the null values with median might erase some information, so to be on the safe side, I created the boolean column with the following code. The boolean column that will contain 'True' for the customers who did not have a null value in its row, and 'False' for customers who contain a null value in its row.
![[Pasted image 20210415193621.png]]

**Now lets take a look at the age column**
![[Pasted image 20210415193916.png]]
![[Pasted image 20210415194511.png]]
The age column has about 25% of missing values. I could not find any clear-cut correlation with the age column after experimenting. After the exploration phase, we can fill null values with a different method besides using the median or mean since it does not provide much value and makes the column less unique. I will recommend building a predictive model to predict the age in the later phases.  The histogram graph does not provide any valuable information.
![[Pasted image 20210415194701.png]]
I used the code above to fill the missing values with the median.  In addition, I added a boolean column that will contain 'True' for the customers who did not have a null value in its row, and 'False' for customers who contain a null value in its row. The reason behind adding the boolean column is that it preserves the information the null values contain.

**Lets take a look at the income null values**
![[Pasted image 20210415200149.png]]

The Histogram for customers who have churn almost looks identical to those from the customers who have not to churn. As previously stated, I will fill the missing values with the median since it is more robust.  Also, I added a boolean column that will contain 'True' for the customers who did not have a null value in its row and 'False' for customers who include a null value in its row.

The following code is the one I used to fill the null values.
![[Pasted image 20210415200622.png]]

**Lets take a look at the Techie, Phone, and TechSupport columns**
All three columns are binary columns containing the values of 'Yes' and 'No.' Techie has ~25% null values, the phone has ~10% null values, and TechSupport has ~10% null values. Let's take a look at a count plot graph for all three of them
![[Pasted image 20210415203612.png]]
![[Pasted image 20210415203638.png]]
I will add a binary column and set null values to 0. I will turn the value 'No' = 1 and 'Yes' = 2.
![[Pasted image 20210415203949.png]]
![[Pasted image 20210415204301.png]]
From my experience in building predictive models, it is best to add a binary column and set 1 to no and 2 to yes. 

**Lets take a look at the tenure null values**
The tenure column has 9% null values in the dataset. I decided to fill the values with the help of the age column. For example, I will grab the median of tenure of customers who are between 20 and 30. Use the median of that age group to fill the values of customers who have null values between 20 and 30. I did the same method for the remaining null values. This way, we keep the column as unique as possible. Also, I added a boolean column that will contain 'True' for the customers who did not have a null value in its row and 'False' for customers who hold a null value in its row.
![[Pasted image 20210416140538.png]]

**Lets take a look at the Bandwidth_GB_Year the last column that has null values**
The column has about 10% null values.
![[Pasted image 20210416142026.png]]

From looking at the histogram, we can see that we have two normal distribution curves. I need to resample the data to create one normal distribution curve and, from there, figure out the mean and median of the column. After sampling the data, this is the distribution curve we get.
![[Pasted image 20210416143004.png]] 
The mean and median of the sample distribution was 3399. The mean of the original dataset for the column Bandwidth_GB_Year was 3399. From that information, I decided to fill the null values with 3399.  Also, I added a boolean column that will contain 'True' for the customers who did not have a null value in its row and 'False' for customers who include a null value in its row. 

![[Pasted image 20210416143745.png]]
We no longer have any more null values and have eight boolean columns to preserve the null value information in the columns.

### Population Outliers
- ![[Pasted image 20210416191330.png]]
- ![[Pasted image 20210416191602.png]]
- From the histogram we can see that the graph has a skew towards the left. From the boxenPlot we can see that we have customers who have a population over 8000. 
- ![[Pasted image 20210416191813.png]]
- I removed 185 rows who had a higher population count that three standard deviation. 
- ![[Pasted image 20210416192101.png]]
- ![[Pasted image 20210416192108.png]]
- You can see that after removing the outliers the BoxenPlot no longer has the dots on the right side.

### Email
![[Pasted image 20210416193827.png]]
![[Pasted image 20210416193839.png]]
From the BoxenPlot and histogram you can come to the conclusion that less than three emails received and more than 20 emails received are anomalies. I then proceeded to remove the anomalies in the email column. After looking at visualizations I decided to look at the email column count
![[Pasted image 20210416194847.png]]
I removed 1% of the rows from the data that had email count anomalies. The rows that were removed also were higher or lower than three standard deviations. After removing the outliers these is how the graphs looked. 
![[Pasted image 20210416194203.png]]
![[Pasted image 20210416194228.png]]

### Income
![[Pasted image 20210416200040.png]]
![[Pasted image 20210416200050.png]]
![[Pasted image 20210416200100.png]]
There are 183 rows that have values outside the three standard deviations. Those rows are consider to be outliers. I will removed them and plot the graphs again.
![[Pasted image 20210416200310.png]]
![[Pasted image 20210416200319.png]]

### Outage_Sec_PerWeek
The quantile 


 ## 2. Justify your methods for mitigating each type of discovered anomaly in the data set.
 ## 3. Summarize the outcome from the implementation of _each_ data-cleaning step.

 ###### 6. Summarize the limitations of the data-cleaning process.
 ###### 7. Discuss how the limitations in part D6 affect the analysis of the question or decision from part A.  
 
#### E.  Apply principal component analysis (PCA) to identify the significant features of the data set by doing the following:
1. List the principal components in the data set.

2. Describe how you identified the principal components of the data set.

3. Describe how the organization can benefit from the results of the PCA

#### G.  Reference the web sources used to acquire segments of third-party code to support the application. Be sure the web sources are reliable.  
 

#### H.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.


  